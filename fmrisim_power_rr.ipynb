{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys \n",
    "import re\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "import os \n",
    "#os.system('/opt/anaconda3/envs/')\n",
    "#from __future__ import print_function\n",
    "#import librosa\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "from datetime import date\n",
    "from copy import deepcopy\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import brainiak\n",
    "import nilearn\n",
    "from nilearn import datasets\n",
    "from nilearn import surface\n",
    "from nilearn import plotting\n",
    "from nilearn.input_data import NiftiMasker, NiftiLabelsMasker,MultiNiftiMasker\n",
    "import nibabel as nib\n",
    "import statsmodels.stats.multitest as smm\n",
    "from brainiak import image, io\n",
    "from brainiak.isc import isc, isfc, permutation_isc\n",
    "#from brainiak.eventseg.event import EventSegment\n",
    "import brainiak.eventseg.event\n",
    "import seaborn as sns \n",
    "import matplotlib\n",
    "import matplotlib.patches as patches\n",
    "import h5py\n",
    "import pickle\n",
    "from scipy import stats\n",
    "import collections\n",
    "from collections import Counter\n",
    "from scipy.stats import pearsonr,spearmanr\n",
    "import brainiak.funcalign.srm\n",
    "import scipy.spatial.distance as sp_distance\n",
    "from brainiak.utils import fmrisim as sim\n",
    "from sklearn.model_selection import LeaveOneOut, KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "import random\n",
    "import deepdish as dd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cython\n",
    "%matplotlib inline\n",
    "%load_ext Cython\n",
    "%load_ext line_profiler\n",
    "\n",
    "\n",
    "#test if updated\n",
    "brainiak.eventseg.event.EventSegment(n_events=53,split_merge = True)\n",
    "\n",
    "smallsize=14; mediumsize=16; largesize=18\n",
    "plt.rc('xtick', labelsize=smallsize); plt.rc('ytick', labelsize=smallsize); plt.rc('legend', fontsize=mediumsize)\n",
    "plt.rc('figure', titlesize=largesize); plt.rc('axes', labelsize=mediumsize); plt.rc('axes', titlesize=mediumsize)\n",
    "sns.set(style = 'white', context='talk', font_scale=1, rc={\"lines.linewidth\": 2})\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    '''\n",
    "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]\n",
    "\n",
    "\n",
    "basepath = '/home/msachs/'\n",
    "outpath = os.path.join(basepath,'fmrisim_music/music_events_rr')\n",
    "\n",
    "\n",
    "##GET SHERLOCK DATA\n",
    "sherlockpath = '/data/Sherlock_movie_files/'\n",
    "fpaths = []\n",
    "for (dirpath,dirnames,filenames) in os.walk(sherlockpath):\n",
    "        fpaths += [os.path.join(dirpath,file) for file in filenames]\n",
    "        \n",
    "fnames = [elem for elem in fpaths if '._' not in elem]\n",
    "fnames.sort(key=natural_keys)\n",
    "nSubj = len(fnames)\n",
    "print('Number of subjects:',nSubj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes fraction of \"ground truth\" bounds are covered by a set of proposed bounds\n",
    "# Returns z score relative to a null distribution via permutation\n",
    "def match_z(proposed_bounds, gt_bounds, num_TRs):\n",
    "    nPerm = 1000\n",
    "    threshold = 3\n",
    "    np.random.seed(0)\n",
    "\n",
    "    gt_lengths = np.diff(np.concatenate(([0],gt_bounds,[num_TRs])))\n",
    "    match = np.zeros(nPerm + 1)\n",
    "    for p in range(nPerm + 1):\n",
    "        gt_bounds = np.cumsum(gt_lengths)[:-1]\n",
    "        for b in gt_bounds:\n",
    "            if np.any(np.abs(proposed_bounds - b) <= threshold):\n",
    "                match[p] += 1\n",
    "        match[p] /= len(gt_bounds)\n",
    "        gt_lengths = np.random.permutation(gt_lengths)\n",
    "    \n",
    "    return match,(match[0]-np.mean(match[1:]))/np.std(match[1:])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython\n",
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolve event timecourses to TR timecourses (timepoint by voxel) for 1.5 TR\n",
    "def regressor_to_TR(E,tr_duration): \n",
    "    T = E.shape[0]\n",
    "    nEvents = E.shape[1]\n",
    "\n",
    "    # HRF (from AFNI)\n",
    "    dt = np.arange(0, 15,tr_duration)\n",
    "    p = 8.6\n",
    "    q = 0.547\n",
    "    hrf = np.power(dt / (p * q), p) * np.exp(p - dt / q)\n",
    "\n",
    "    # Convolve event matrix to get design matrix\n",
    "    design_seconds = np.zeros((T, nEvents))\n",
    "    for e in range(nEvents):\n",
    "        design_seconds[:, e] = np.convolve(E[:, e], hrf)[:T]\n",
    "    \n",
    "    gt_sig_conv = design_seconds\n",
    "    return gt_sig_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for generating signal for subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import numpy as np\n",
    "#import random\n",
    "def gtsignal(data,nSubj,ntr,bounds): #we want to input the masked fmri signal that we are using to get the gtsignal (nvox,ntr,nSubj)\n",
    "    error = 'False'\n",
    "    nvox = data.shape[0]\n",
    "    onsets = [x-4 for x in bounds]\n",
    "    onsets = np.append(0,onsets)\n",
    "    onsets = np.append(onsets,ntr)\n",
    "    sim_conv = np.zeros((ntr,nvox,nSubj))\n",
    "    sim_conv_wnoise = np.zeros((ntr,nvox,nSubj))\n",
    "    sim_data = np.zeros((ntr,nvox,nSubj))\n",
    "    gt_brain = np.zeros((nvox,ntr,nSubj))\n",
    "    for s in range(0,nSubj):\n",
    "        gt_sig = np.zeros((nvox,ntr))\n",
    "        randlist = np.arange(5,ntr-1)\n",
    "        for e,t in enumerate(onsets):\n",
    "            if t < ntr:\n",
    "                random.shuffle(randlist)\n",
    "                randtr = randlist[e]\n",
    "                randlist = np.delete(randlist,np.where(randlist == randtr)[0][0],0)\n",
    "                sub = random.choice(goodsublist)\n",
    "                while data[:,randtr,sub].sum() == 0:\n",
    "                    randtr = random.randint(0, vmpfc_brain.shape[3])\n",
    "                if e == 0:\n",
    "                    gt_sig[:,0] = data[:,randtr,sub]\n",
    "                if e == len(bounds):\n",
    "                    state = np.repeat(data[:,randtr,sub,np.newaxis], int(onsets[e+1]+1) - int(t+1)-1, axis=1)\n",
    "                else:\n",
    "                    state = np.repeat(data[:,randtr,sub,np.newaxis], int(onsets[e+1]+1) - int(t+1), axis=1)\n",
    "                \n",
    "                gt_sig[:,int(t+1):int(onsets[e+1]+1)] = state\n",
    "                \n",
    "        #check quickly\n",
    "        if (len(np.where(np.diff(gt_sig[1,:]))[0])) != len(bounds):\n",
    "            print('Something went wrong with gt data',len(np.where(np.diff(gt_sig[1,:]))[0]))\n",
    "            error = 'TRUE'\n",
    "            break\n",
    "\n",
    "        #gt_sig_conv = brainiak.utils.fmrisim.convolve_hrf(stimfunction=gt_sig.T,tr_duration=1.5,temporal_resolution=1,scale_function=1)\n",
    "        gt_sig_conv = regressor_to_TR(gt_sig.T,tr_duration)\n",
    "        sim_conv[:,:,s] = gt_sig_conv\n",
    "        sim_data[:,:,s] = gt_sig.T\n",
    "        #sim_conv_wnoise[:,:,s] = 1*gt_sig_conv + weight*noise_sherlock_zscore[0:ntr,:]\n",
    "        \n",
    "    return sim_conv,sim_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get only voxels in the region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many TRs are there?\n",
    "nii = nib.load(fnames[0])\n",
    "dimsize = nii.header.get_zooms()\n",
    "tr_duration = 1.5 #dimsize[3]\n",
    "#real_brain = nii.get_fdata()\n",
    "real_brain = nii.get_fdata(dtype=np.float32)\n",
    "#real_brain = nii.dataobj\n",
    "dimensions = np.array(real_brain.shape[0:3])  # What is the size of the brain\n",
    "dim = real_brain.shape\n",
    "ntr_sherlock = nii.shape[3]\n",
    "affine = nii.affine\n",
    "header = nii.header\n",
    "\n",
    "#load VMPFC ROI\n",
    "vmpfcfile = os.path.join(outpath,'chang_pfc_cluster_labels_k3.nii.gz')\n",
    "vmpfc = nib.load(vmpfcfile).get_fdata()\n",
    "vmpfc_img = nib.load(vmpfcfile)\n",
    "vmpfc_resamp = nilearn.image.resample_to_img(vmpfc_img, nii,interpolation='nearest') \n",
    "vmpfc_roi = vmpfc_resamp.get_fdata()\n",
    "vmpfc_roi[np.where(vmpfc_resamp.get_fdata() != 3)] = 0\n",
    "\n",
    "mask_outname = outpath + '/vmpc_chang_downsample.nii.gz'\n",
    "if not os.path.exists(mask_outname):\n",
    "    mask_nifti = nib.Nifti1Image(vmpfc_mask, affine,header)\n",
    "    print('Saving',mask_outname)\n",
    "    nib.save(mask_nifti,mask_outname)\n",
    "else:\n",
    "    vmpfc_resamp = nib.load(mask_outname) \n",
    "\n",
    "#afni 3dresample \n",
    "#'3dresample -inset mask_original -master functional_data -prefix mask_resampled' -rmode NN\n",
    "\n",
    "vmpfc_resamp.get_data().max() #this is the resampled 1,2,3 vmpfc image in nibabel loaded image\n",
    "vmpfc_dat = vmpfc_resamp.get_data() #this is the resampled 1,2,3 vmpfc image in data\n",
    "vmpfc_mask = np.logical_not(vmpfc_dat != 3) #boolean mask\n",
    "nvox = np.count_nonzero(vmpfc_mask)\n",
    "print('Number of voxels...',nvox)\n",
    "\n",
    "#mean_img = nilearn.image.image.mean_img(fnames[0])\n",
    "mean_data = real_brain.mean(axis = 3)\n",
    "mean_img = nib.Nifti1Image(mean_data, affine=affine)\n",
    "plotting.plot_roi(vmpfc_resamp, bg_img=mean_img,title=\"plot_vmpfc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the noise in a ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the continuous mask from the voxels\n",
    "mask, template = sim.mask_brain(volume=vmpfc_brain,mask_self=True)\n",
    "mask2, template2 = sim.mask_brain(volume=vmpfc_only,mask_self=True)\n",
    "mask2 = mask.astype('float32')\n",
    "\n",
    "#calculating the noise parameter\n",
    "match_noise = int(1) #Whether or not to fit the noise\n",
    "print('Determining noise')\n",
    "noise_dict = {'voxel_size': [dimsize[0], dimsize[1], dimsize[2]], 'matched': match_noise}\n",
    "noise_dict = sim.calc_noise(volume=vmpfc_brain,\n",
    "                            mask=mask,\n",
    "                            template=template,\n",
    "                            noise_dict=noise_dict,\n",
    "                            )\n",
    "print('Noise parameters of the data were estimated as follows:')\n",
    "print('SNR: ' + str(noise_dict['snr']))\n",
    "print('SFNR: ' + str(noise_dict['sfnr'])) #Signal-to-Fluctuation-Noise Ratio aka temporal signal-to-noise, reflects how much temporal variation there is in brain voxels\n",
    "print('FWHM: ' + str(noise_dict['fwhm'])) #5-mm full width-half maximum spatial kernel was used in Antony\n",
    "\n",
    "\n",
    "print('Generating noise')\n",
    "noise_sherlock = sim.generate_noise(dimensions=dimensions,\n",
    "                           stimfunction_tr=[0] * ntr_sherlock,\n",
    "                           tr_duration=int(tr_duration),\n",
    "                           template=template,\n",
    "                           mask=mask,\n",
    "                           noise_dict=noise_dict,\n",
    "                           )\n",
    "\n",
    "noise_newdata = sim.generate_noise(dimensions=dimensions,\n",
    "                           stimfunction_tr=[0] * ntr,\n",
    "                           tr_duration=int(tr_duration),\n",
    "                           template=template,\n",
    "                           mask=mask,\n",
    "                           noise_dict=noise_dict,\n",
    "                           )\n",
    "\n",
    "#Save as h5 file\n",
    "with h5py.File(os.path.join(outpath + 'noise_newdata.h5')) as hf:\n",
    "    hf.require_dataset('noise_newdata', data=noise_newdata,shape = noise_newdata.shape, dtype = 'float64')\n",
    "    \n",
    "######### GENERATE NOISE FOR 50 subjects in a loop\n",
    "all_noise_dicts2 = {}\n",
    "for s,f in enumerate(fnames[2:10]):\n",
    "    nib_nii = nib.load(f)\n",
    "    vmpfc_brain = nib_nii.get_data()\n",
    "    vmpfc_brain[np.where(vmpfc_dat != 3)] = 0 #this is one brain data, only voxels in the vmpfc in 4D\n",
    "    vmpfc_only = vmpfc_brain[min(xx):max(xx), min(yy):max(yy), min(zz):max(zz), :]\n",
    "    \n",
    "    # Generate the continuous mask from the voxels\n",
    "    mask, template = sim.mask_brain(volume=vmpfc_brain,mask_self=True)\n",
    "    #mask2, template2 = sim.mask_brain(volume=vmpfc_only,mask_self=True)\n",
    "\n",
    "    #calculating the noise parameter\n",
    "    match_noise = int(1) #Whether or not to fit the noise\n",
    "    print('Determining noise')\n",
    "    noise_dict = {'voxel_size': [dimsize[0], dimsize[1], dimsize[2]], 'matched': match_noise}\n",
    "    #noise_dict2 = {'voxel_size': [vmpfc_only.shape[0], vmpfc_only.shape[1], vmpfc_only.shape[2]], 'matched': match_noise}\n",
    "    calc_noise = sim.calc_noise(volume=vmpfc_only,mask=mask2,template=template2,noise_dict=noise_dict)\n",
    "    print('Noise parameters of the data were estimated as follows:')\n",
    "    print('SNR: ' + str(calc_noise['snr']))\n",
    "    print('SFNR: ' + str(calc_noise['sfnr'])) #Signal-to-Fluctuation-Noise Ratio aka temporal signal-to-noise, reflects how much temporal variation there is in brain voxels\n",
    "    print('FWHM: ' + str(calc_noise['fwhm'])) #5-mm full width-half maximum spatial kernel was used in Antony\n",
    "    all_noise_dicts2[s] = calc_noise\n",
    "\n",
    "calc_noise_avg = calc_noise\n",
    "for s in range(1,10):\n",
    "    print(all_noise_dicts[s])\n",
    "    all_noise_dicts[1:10]['fwhm']\n",
    "    all_noise_dicts[s]['sfnr']\n",
    "    all_noise_dicts[s]['snr']\n",
    "#generate noise for 50 different subjects\n",
    "noise_library = {}\n",
    "\n",
    "for r in range(50):\n",
    "#     try:\n",
    "#         noise_library[r]\n",
    "#     except KeyError:\n",
    "#         noise_library[r] = {}\n",
    "    #filename = outpath + '/sub'+str(r+1)+'_noise_1976tr.h5' \n",
    "    filename = outpath + '/sub'+str(r+1)+'_noise_1350tr.h5' \n",
    "    if not os.path.exists(filename):\n",
    "    #if len(noise_library[r]) == 0:\n",
    "        print('Generating noise for',r)\n",
    "        #noise_library[r] = {}\n",
    "        start = time.process_time()\n",
    "        noise = sim.generate_noise(dimensions,stimfunction_tr=[0] * ntr,\n",
    "                               tr_duration=1.5,\n",
    "                               template=template,\n",
    "                               mask=mask,\n",
    "                               noise_dict=calc_noise,\n",
    "                               iterations = [0,0]\n",
    "                               )\n",
    "\n",
    "        print('Saving',filename)\n",
    "        with h5py.File(filename) as hf:\n",
    "            hf.require_dataset('noise', data=noise,shape = noise.shape, dtype = 'float64')\n",
    "        \n",
    "        elapsed = (time.process_time() - start)\n",
    "        print('Time elapsed is...',elapsed)\n",
    "        del noise \n",
    "        \n",
    "        #Save as h5 file\n",
    "        noise_library[r] = noise\n",
    "        del noise \n",
    "        \n",
    "    if not os.path.exists(filename):\n",
    "        print('Saving',filename)\n",
    "        dd.io.save(filename,noise_library[r])\n",
    "        \n",
    "filename = os.path.join(outpath + 'noise_library_10ss_1976trs.h5')\n",
    "dd.io.save(filename,noise_library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the HMM on the original data to determine z statistic in the VMPFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_onsets = [26, 35, 56, 72, 86, 108, 131, 143, \n",
    "          157, 173, 192, 204, 226, 313, 362, \n",
    "          398, 505, 526, 533, 568, 616, 634, \n",
    "          678, 696, 747, 780, 870, 890, 945, \n",
    "          971, 1008, 1076, 1104, 1139, 1222, \n",
    "          1268, 1350, 1360, 1435, 1470, 1538, \n",
    "          1580, 1592, 1666, 1692, 1702, 1735, \n",
    "          1762, 1804, 1825, 1856, 1936, 1946]\n",
    "\n",
    "\n",
    "#mask all subject data\n",
    "masked_data = np.zeros((nvox,ntr_sherlock,nSubj))\n",
    "for s,fname in enumerate(fnames):\n",
    "    print(s,fname)\n",
    "    brain_masked = nib.load(fname).get_data()\n",
    "    #brain_masked[vmpfc_dat == 3]\n",
    "    temp = brain_masked[vmpfc_mask]\n",
    "    \n",
    "    #check if all zero somehwere\n",
    "    rows = np.count_nonzero(np.all((temp == 0), axis=1)) #37 rows\n",
    "    cols = np.count_nonzero(np.all((temp == 0), axis=0))\n",
    "    print(rows,cols)\n",
    "#     if rows > 0:\n",
    "#         for i in range(len(np.all((temp == 0), axis=1))):\n",
    "#             if np.all((temp == 0), axis=1)[i]:\n",
    "#                 print('Row: ', i)\n",
    "\n",
    "    masked_data[:,:,s] = brain_masked[vmpfc_mask]\n",
    "    \n",
    "goodsublist = [0,2,4,7,9,10,11,13,14,15,16]\n",
    "\n",
    "#Save as h5 file\n",
    "# with h5py.File(os.path.join(outpath + 'vmpfc_masked_data_17ss.h5')) as hf:\n",
    "#     hf.require_dataset('masked_data', data=masked_data,shape = masked_data.shape, dtype = 'float64')\n",
    "\n",
    "#Save using dd\n",
    "filename = os.path.join(outpath + '/vmpfc_masked_data_17ss_dd.h5')\n",
    "dd.io.save(filename,masked_data)\n",
    "\n",
    "\n",
    "##BALDASSANO WAY\n",
    "hmm = brainiak.eventseg.event.EventSegment(n_events=len(cb_onsets)+1,split_merge = True)\n",
    "hmm.fit(masked_data.mean(2).T) #timepoin by voxel\n",
    "events = np.argmax(hmm.segments_[0], axis=1)\n",
    "bounds = np.where(np.diff(events))[0]\n",
    "_,zstat_orig = match_z(bounds, cb_onsets, ntr_sherlock)\n",
    "pval_orig = stats.norm.sf(zstat_orig)\n",
    "print(zstat_orig,pval_orig)\n",
    "\n",
    "# Plot probability of being in each event at each timepoint\n",
    "plt.figure(figsize = (12, 6))\n",
    "plt.matshow(hmm.segments_[0].T, aspect='auto')\n",
    "plt.gca().xaxis.tick_bottom()\n",
    "plt.colorbar()\n",
    "plt.title('Event probability')\n",
    "\n",
    "# Plot boundaries as boxes on top of timepoint correlation matrix\n",
    "def plot_tt_similarity_matrix(ax, data_matrix, bounds, n_TRs, title_text):\n",
    "    \n",
    "    ax.imshow(np.corrcoef(data_matrix), cmap = 'viridis',vmax = 0.3, vmin = -0.3)\n",
    "    ax.set_title(title_text)\n",
    "    ax.set_xlabel('TR')\n",
    "    ax.set_ylabel('TR')\n",
    "    \n",
    "    # plot the boundaries \n",
    "    bounds_aug = np.concatenate(([0], bounds, [n_TRs]))\n",
    "    \n",
    "    for i in range(len(bounds_aug) - 1):\n",
    "        rect = patches.Rectangle(\n",
    "            (bounds_aug[i], bounds_aug[i]),\n",
    "            bounds_aug[i+1] - bounds_aug[i],\n",
    "            bounds_aug[i+1] - bounds_aug[i],\n",
    "            linewidth = 2, edgecolor = 'w',facecolor = 'none'\n",
    "        )\n",
    "        ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLot the simulated data and noise\n",
    "#sim_data_new = 1050TR, not convolved\n",
    "#fmrisim_new = 1050TR, convolved\n",
    "#fmrisim = 1976TR, convolved (Baldassano events)\n",
    "#subnoise_mask_zscore = 1976TR noise in VMPFC for one subject\n",
    "#signoise_sherlock = (0.042*fmrisim) + 1*subnoise_mask_zscore\n",
    "#signoise_new = (0.042*fmrisim_new) + 1*subnoise_mask_zscore[0:ntr,:]\n",
    "\n",
    "#PLot the sherlock simulated, noise, and signal+noise (20 voxels, 100 time points)\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.subplot(2,1,1)\n",
    "t = fmrisim[5:200,0:20] #timepoint by voxel by subject\n",
    "plt.imshow(t.T)\n",
    "plt.ylabel('Voxels')\n",
    "plt.xlabel('Time')\n",
    "plt.colorbar()\n",
    "plt.subplot(2,1,2)\n",
    "z = subnoise_mask_zscore[5:200,0:20]\n",
    "plt.imshow(z.T)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Voxels')\n",
    "plt.colorbar()\n",
    "# plt.subplot(3,1,3)\n",
    "# x = signoise_sherlock[5:200,0:20]\n",
    "# plt.imshow(x.T)\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Voxels')\n",
    "# plt.colorbar()\n",
    "figname = '/home/msachs/fmrisim_music/sherlock_sig_noise.png'\n",
    "plt.savefig(figname, dpi = 300)\n",
    "\n",
    "\n",
    "# Plot the pattern of activity for our new signal voxels at each timepoint\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.subplot(2,1,1)\n",
    "t = fmrisim_new[5:200,0:20] #timepoint by voxel by subject\n",
    "plt.imshow(t.T)\n",
    "plt.ylabel('Voxels')\n",
    "plt.xlabel('Time')\n",
    "plt.colorbar()\n",
    "plt.subplot(2,1,2)\n",
    "z = signoise_new[5:200,0:20]\n",
    "plt.imshow(z.T)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Voxels')\n",
    "plt.colorbar()\n",
    "# plt.subplot(3,1,3)\n",
    "# x = signoise_sherlock[5:200,0:20]\n",
    "# plt.imshow(x.T)\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Voxels')\n",
    "# plt.colorbar()\n",
    "figname = '/home/msachs/fmrisim_music/newdesign_sig_noise_1050tr.png'\n",
    "plt.savefig(figname, dpi = 300)\n",
    "\n",
    "\n",
    "#timepoint by timepoint no bounds\n",
    "f, ax = plt.subplots(1,1, figsize = (10,8))\n",
    "ax.imshow(np.corrcoef(mean_sim_w), cmap = 'viridis')\n",
    "ax.set_title(title_text)\n",
    "ax.set_xlabel('TR')\n",
    "ax.set_ylabel('TR')\n",
    "\n",
    "#Time point by timepoint with bounds \n",
    "f, ax = plt.subplots(1,1, figsize = (10,8))\n",
    "title_text = '''\n",
    "TR-TR pattern correlation\n",
    "'''\n",
    "#plot_tt_similarity_matrix(ax, signoise_sherlock, cb_onsets, ntr_sherlock, title_text)\n",
    "\n",
    "plot_tt_similarity_matrix(ax, signoise_new, new_onsets, ntr, title_text)\n",
    "f.savefig('/home/msachs/fmrisim_music/vmpfc_tt_newdesign_1050tr.png')\n",
    "\n",
    "                  \n",
    "#Save as h5 file\n",
    "with h5py.File(os.path.join(outpath + 'gt_signal_sherlock_conv.h5')) as hf:\n",
    "    hf.require_dataset('sherlocksim_conv', data=sherlocksim_conv,shape = sherlocksim_conv.shape, dtype = 'float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try reading hf5 files \n",
    "try:\n",
    "    print(noise_sherlock.shape)\n",
    "    print(sherlocksim_conv)\n",
    "except NameError:\n",
    "    with h5py.File(os.path.join(outpath + '/noise_sherlock.h5')) as hf:\n",
    "        noise_sherlock = np.array(hf['noise'])\n",
    "    print(noise_sherlock.shape)\n",
    "    \n",
    "    with h5py.File(os.path.join(outpath + '/.h5')) as hf:\n",
    "        sherlocksim_conv = np.array(hf['sherlocksim_conv'])\n",
    "    print(sherlocksim_conv.shape)\n",
    "    \n",
    "                   \n",
    "#zscore the noise\n",
    "noise_mask = noise_sherlock[vmpfc_mask].T\n",
    "noise_sherlock_zscore = stats.zscore(noise_mask,axis = 0, ddof=1)\n",
    "noise_sherlock_zscore[:,1].mean()\n",
    "\n",
    "\n",
    "hmmfits_sherlock = {}\n",
    "hmm_w= brainiak.eventseg.event.EventSegment(n_events=len(cb_onsets)+1,split_merge = True)\n",
    "\n",
    "#try it first on the convolved data \n",
    "hmm_w.fit(sherlocksim_conv[:,:,:].mean(2))\n",
    "events_w = np.argmax(hmm_w.segments_[0], axis=1)\n",
    "bounds_w = np.where(np.diff(events_w))[0]\n",
    "match_perm,zstat = match_z(bounds_w,cb_onsets,ntr_sherlock)\n",
    "\n",
    "weights = np.logspace(0,1,10,endpoint=True)\n",
    "#weights = list(map(lambda x: 10 ** x, range(5)))\n",
    "weights = 1/np.array(weights)\n",
    "weights = [.45,.44,.43,.42]\n",
    "weights = [.70]\n",
    "sherlocksim_weight = {w:np.zeros((ntr_sherlock,nvox,nSubj)) for w in weights}\n",
    "for n,w in enumerate(weights):\n",
    "    hmmfits_sherlock[w] = {}\n",
    "    for s in range(0,nSubj):\n",
    "        wsum = 1*sherlocksim_conv[:,:,s] + w*noise_sherlock_zscore\n",
    "        sherlocksim_weight[w][:,:,s] = wsum\n",
    "        \n",
    "    \n",
    "    #fit the hmm\n",
    "    hmm_w.fit(sherlocksim_weight[w][:,:,:].mean(2))\n",
    "    events_w = np.argmax(hmm_w.segments_[0], axis=1)\n",
    "    bounds_w = np.where(np.diff(events_w))[0]\n",
    "    match_perm,zstat = match_z(bounds_w,cb_onsets,ntr_sherlock)\n",
    "    pval = stats.norm.sf(zstat)\n",
    "    hmmfits_sherlock[w]['match_perm'] = match_perm\n",
    "    hmmfits_sherlock[w]['events'] = events_w \n",
    "    hmmfits_sherlock[w]['zstat'] = zstat\n",
    "    hmmfits_sherlock[w]['pval'] = pval\n",
    "    print(n,'downweighting to ',w,'zstat is',round(zstat,4),'compared to',round(zstat_orig,4))\n",
    "    if abs(zstat-zstat_orig) < .3:\n",
    "        print('Found the right weight!')\n",
    "        break \n",
    "\n",
    "###########################\n",
    "##### #Do this in a loop to make sure (06-20)\n",
    "###########################\n",
    "\n",
    "#load real masked data\n",
    "goodsublist = [0,2,4,7,9,10,11,13,14,15,16]\n",
    "masked_data_list = dd.io.load(os.path.join(outpath + '/vmpfc_masked_data_17ss_dd.h5'))\n",
    "masked_data = np.array(masked_data_list)\n",
    "masked_data.shape #voxel by time point by subject\n",
    "\n",
    "##Getting ground truth data\n",
    "fmrisim,sim_data = gtsignal(masked_data,17,ntr_sherlock,cb_onsets)\n",
    "#fmrisim = np.squeeze(fmrisim)\n",
    "#sim_data = np.squeeze(sim_data)\n",
    "fmrisim = fmrisim.mean(axis = 2)\n",
    "\n",
    "hmm = brainiak.eventseg.event.EventSegment(n_events=len(cb_onsets)+1,split_merge = True)\n",
    "hmmfits = {}\n",
    "samples = 40\n",
    "\n",
    "noise_samples = {s:np.zeros((ntr_sherlock,nvox,nSubj)) for s in range(samples)}\n",
    "zstats = {}\n",
    "zstats_075 = zstats\n",
    "weights = [.1,0.075,0.065.06,0.05]\n",
    "weights= [0.05,0.045]\n",
    "zstats2 = {w:np.zeros((samples)) for w in weights}\n",
    "zstats2[w] = np.zeros((samples))\n",
    "                     \n",
    "for d in range(0,samples):\n",
    "    print('Sample',d)\n",
    "    if noise_samples[d].sum() == 0:\n",
    "        randsubs = np.arange(3,50)\n",
    "        #load 17 noise subjects \n",
    "        print('Getting noise for',nSubj,'sample',d)\n",
    "        for s in range(nSubj):\n",
    "            random.shuffle(randsubs)\n",
    "            randsub = randsubs[s]\n",
    "            randsubs = np.delete(randsubs,np.where(randsubs == randsub)[0][0],0)\n",
    "\n",
    "            fname = outpath + '/sub'+str(randsub)+'_noise_1976tr.h5'\n",
    "            with h5py.File(fname) as hf:\n",
    "                subnoise = np.array(hf['noise'])\n",
    "            #print(s,fname,subnoise.shape)\n",
    "            subnoise_mask = subnoise[vmpfc_mask].T\n",
    "            noise_samples[d][:,:,s] = stats.zscore(subnoise_mask,axis = 0, ddof=1)\n",
    "    else:\n",
    "        for n,w in enumerate(weights):\n",
    "            sherlocksim_weight = {s:np.zeros((ntr_sherlock,nvox,nSubj)) for s in range(samples)}\n",
    "            print('Adding weights',w)\n",
    "            for s in range(nSubj):\n",
    "                sherlocksim_weight[d][:,:,s] = w*fmrisim + 1*noise_samples[d][:,:,s]\n",
    "\n",
    "            #run the hmm for this loop\n",
    "            print('Running HMM...')\n",
    "            hmm.fit(sherlocksim_weight[d][:,:,:].mean(2)) #timepoint by voxel\n",
    "            events = np.argmax(hmm.segments_[0], axis=1)\n",
    "            bounds = np.where(np.diff(events))[0]\n",
    "\n",
    "            match_perm,zstat = match_z(bounds,cb_onsets,ntr_sherlock)\n",
    "            pval = stats.norm.sf(zstat)\n",
    "            print(d,w,'with',nSubj,'subjects, zstat is',round(zstat,4),'pval is',round(pval,4))\n",
    "            zstats2[w][d] = zstat\n",
    "zlist = []\n",
    "for d in range(samples): \n",
    "    print(d,zstats2[w][d])\n",
    "    zlist.append(zstats2[w][d]) \n",
    "print(np.mean(zlist),zstat_orig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate the signal for new study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate event time course\n",
    "# Set up stimulus event time course parameters\n",
    "try: \n",
    "    masked_data\n",
    "except NameError:\n",
    "    masked_data_list = dd.io.load(os.path.join(outpath + '/vmpfc_masked_data_17ss_dd.h5'))\n",
    "    masked_data = np.array(masked_data_list)\n",
    "goodsublist = [0,2,4,7,9,10,11,13,14,15,16]\n",
    "print(masked_data.shape) #voxel by time point by subject\n",
    "\n",
    "conditions = 7\n",
    "nsec = 45*5*conditions #increase this to 7 examples of each\n",
    "ntr = int(nsec/1.5) #5 emos, 6exampels of each, 1976 for Baldassano\n",
    "event_duration = 30  # How long is each event (in TR it's 30, in sec its 45)\n",
    "isi = 0  # What is the time between each event\n",
    "burn_in = 0  # How long before the first event\n",
    "num_events = int(ntr/event_duration)\n",
    "\n",
    "#define new bounds\n",
    "osets = np.arange(event_duration,ntr,event_duration)\n",
    "new_onsets = np.zeros((num_events-1),dtype = 'int64')\n",
    "for i,o in enumerate(osets):\n",
    "    new_onsets[i] = int(o + random.randint(-10,10))\n",
    "    #print(o,new_onsets[i])\n",
    "\n",
    "\n",
    "##Getting ground truth data\n",
    "fmrisim_new,sim_data_new = gtsignal(masked_data,40,ntr,new_onsets)\n",
    "#fmrisim_new = fmrisim_new.mean(axis = 2)\n",
    "#sim_data_new  = sim_data_new.mean(axis = 2)\n",
    "print('Simulating new data with',num_events, 'events')\n",
    "\n",
    "#in case restart\n",
    "nvox = masked_data.shape[0]\n",
    "print(fmrisim_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run HMM with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSubjlist = [25,30,35,40]\n",
    "power = 100\n",
    "pvals = {ns:np.zeros((power)) for ns in nSubjlist}\n",
    "w = 0.042\n",
    "#nSubjlist = [17,20,25,30,35,40]\n",
    "\n",
    "hmm_new = brainiak.eventseg.event.EventSegment(n_events=len(new_onsets)+1,split_merge = True)\n",
    "for ns in nSubjlist:\n",
    "    power_samples = {p:np.zeros((ntr,nvox,ns)) for p in range(power)}\n",
    "    newsim_weight = {p:np.zeros((ntr,nvox,ns)) for p in range(power)}\n",
    "    for p in np.arange(0,power):\n",
    "        if pvals[ns][p] != 0:\n",
    "            continue\n",
    "        randsubs = np.arange(1,50)\n",
    "        #load 17 noise subjects \n",
    "        print('Getting noise for',ns,'power test',p)\n",
    "        for s in np.arange(0,ns):\n",
    "            if power_samples[p][:,:,s].sum() != 0:\n",
    "                continue\n",
    "            random.shuffle(randsubs)\n",
    "            randsub = randsubs[0]\n",
    "            randsubs = np.delete(randsubs,np.where(randsubs == randsub)[0][0],0)\n",
    "\n",
    "            fname = outpath + '/sub'+str(randsub)+'_noise_1350tr.h5'\n",
    "            with h5py.File(fname,'r') as hf:\n",
    "                #subnoise = np.array(hf['noise'])\n",
    "                subnoise = hf['noise'][:,:,:,0:ntr]\n",
    "                #subnoise2 = hf['noise'][:]\n",
    "            subnoise_mask = subnoise[vmpfc_mask].T\n",
    "            del subnoise\n",
    "            power_samples[p][:,:,s] = stats.zscore(subnoise_mask,axis = 0, ddof=1)\n",
    "            print(s,fname,power_samples[p][:,:,s].shape)\n",
    "            del subnoise_mask\n",
    "            print(p,'Adding weights',w)\n",
    "            newsim_weight[p][:,:,s] = w*fmrisim_new + 1*power_samples[p][:,:,s]\n",
    "\n",
    "        #run the hmm for this loop\n",
    "        print('Running HMM...')\n",
    "        hmm_new.fit(newsim_weight[p][:,:,:].mean(2)) #timepoint by voxel\n",
    "        events = np.argmax(hmm_new.segments_[0], axis=1)\n",
    "        bounds = np.where(np.diff(events))[0]\n",
    "\n",
    "        match_perm_new,zstat_new = match_z(bounds,new_onsets,ntr)\n",
    "        pval_new = stats.norm.sf(zstat_new)\n",
    "        print(p,w,'with',ns,'subjects, zstat is',round(zstat_new,4),'pval is',round(pval_new,4))\n",
    "        pvals[ns][p] = pval_new\n",
    "    del newsim_weight\n",
    "\n",
    "    beta = np.count_nonzero(pvals[ns] < 0.05)/power    \n",
    "    print(ns,power,np.mean(bexrta))\n",
    "    pvals[ns]['beta'] = beta\n",
    "\n",
    "    #save the pval output for power (07-2020)\n",
    "    outh5 = '/home/msachs/fmrisim_music/power_pvals_042w.h5'\n",
    "    dd.io.save(outh5,pvals)\n",
    "\n",
    "#load\n",
    "h5file = '/home/msachs/fmrisim_music/power_pvals_042w_1050tr_py.h5'\n",
    "pvals = dd.io.load(h5file)\n",
    "len(np.where(pvals[40][40] < 0.005)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power analysis for encoding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "nPerm = 1000\n",
    "#estimator = Ridge(alpha=100.)\n",
    "estimator = Ridge(alpha=0)\n",
    "cv = KFold(n_splits=5)\n",
    "\n",
    "nfeats = 20\n",
    "feature_space = np.zeros((ntr,nfeats))\n",
    "noise=np.random.normal(0,1,ntr) \n",
    "weights = [.1,1,5,10,100]\n",
    "\n",
    "encode_sim = {w:np.zeros((100,4)) for w in weights}\n",
    "voxels = np.arange(0,nvox).tolist()\n",
    "power_samples = 100\n",
    "for power in range(0,power_samples):\n",
    "    randvox = random.choice(voxels)\n",
    "    voxels.remove(randvox)\n",
    "    \n",
    "    voxel = sim_data_new.mean(axis = 2)[:,randvox]\n",
    "    voxel_conv = fmrisim_new[:,randvox,:]\n",
    "    bounds = [x-3 for x in new_onsets]\n",
    "    for w in weights: \n",
    "        wrange = np.linspace(w,w-.5,nfeats)\n",
    "        for f in range(nfeats):\n",
    "            feature_space[:,f] = voxel + wrange[f]*noise\n",
    "        \n",
    "        feature_space_conv = regressor_to_TR(feature_space,tr_duration)\n",
    "        print(voxel.shape,voxel_conv.shape,feature_space_conv.shape,len(bounds))\n",
    "        \n",
    "        print('Running permutations for power test',power,'weight',w)\n",
    "        #result = dask.delayed(ridge_perm,nout=4)(voxel,feature_space_conv,voxel_conv,bounds,num_events,nPerm)\n",
    "        \n",
    "        #futures = dask.persist(*result)  # trigger computation in the background\n",
    "        #%time r,pval_r,r2,pval_r2  = dask.compute(*futures)\n",
    "        %time r,pval_r,r2,pval_r2 = ridge_perm(voxel,feature_space_conv,voxel_conv,bounds,num_events,nPerm)\n",
    "\n",
    "\n",
    "        print(round(r,3),round(pval_r,3),round(r2,3),round(pval_r2,3))\n",
    "        encode_sim[w][power,0] = r\n",
    "        encode_sim[w][power,1] = pval_r\n",
    "        encode_sim[w][power,2] = r2\n",
    "        encode_sim[w][power,3] = pval_r2\n",
    "\n",
    "    filename = os.path.join(outpath + '/encode_sim_power_100_ols.h5')\n",
    "    print(\"Saving as\",filename)\n",
    "    dd.io.save(filename,encode_sim)\n",
    "\n",
    "# if not os.path.exists(filename):\n",
    "#     dd.io.save(filename,encode_sim)\n",
    "# else:\n",
    "#     encode_sim = dd.io.load(filename)\n",
    "#get power\n",
    "for w in weights: \n",
    "    temp = encode_sim[w]\n",
    "    sigs = len(np.where(temp[:,1] < 0.05)[0])\n",
    "    beta = sigs/power_samples\n",
    "    r_avg = np.mean(temp[:,0])\n",
    "    r2_avg = np.mean(temp[:,2])\n",
    "    print(w,beta,round(r_avg,4),round(r2_avg,4))\n",
    "\n",
    "#plot noise\n",
    "# feat_noise = feature + 2*noise\n",
    "# X=np.arange(0,ntr)\n",
    "# plt.scatter(X,feature,label= 'orig')\n",
    "# plt.scatter(X,feat_noise,label='noise',marker='+') \n",
    "# spearmanr(feature,feat_noise)[0]\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Model Power Analysis (Try 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "from dask.distributed import Client, progress\n",
    "\n",
    "\n",
    "def ridge_perm(voxel,feature_space,voxel_conv,bounds,num_events,nPerm):\n",
    "    perm_scores = []\n",
    "    perm_corrs = []\n",
    "    nSubj = voxel_conv.shape[1]\n",
    "    for p in range(nPerm + 1):\n",
    "        cv_scores = []\n",
    "        cv_corrs = []\n",
    "        for left_out in range(fmrisim_new.shape[2]):\n",
    "            voxel_conv_mean = voxel_conv[:,np.arange(nSubj) != left_out].mean(1)\n",
    "            scores = []\n",
    "            corrs = []\n",
    "            for train, test in cv.split(X=feature_space):\n",
    "                # we train the Ridge estimator on the training set\n",
    "                # and predict the fMRI activity for the test set\n",
    "                predictions = Ridge(alpha=0.).fit(\n",
    "                feature_space[train], voxel_conv_mean[train]).predict(feature_space[test])\n",
    "\n",
    "                #we compute how much variance our encoding model explains in each voxel\n",
    "                scores.append(r2_score(voxel_conv_mean[test], predictions,\n",
    "                                       multioutput='raw_values')[0])\n",
    "                corrs.append(spearmanr(voxel_conv_mean[test],predictions)[0])\n",
    "            \n",
    "            cv_scores.append(np.mean(scores))\n",
    "            cv_corrs.append(np.mean(corrs))\n",
    "        \n",
    "        perm_scores.append(np.mean(cv_scores))\n",
    "        perm_corrs.append(np.mean(cv_corrs))\n",
    "        \n",
    "        #Permutation\n",
    "        chunks = np.split(voxel,np.array(bounds))\n",
    "        feature_rand = []\n",
    "        for i,n in enumerate(np.random.permutation(np.arange(0,num_events))): \n",
    "            feature_rand.extend(chunks[n])\n",
    "\n",
    "        voxel = np.array(feature_rand)\n",
    "        bounds_perm = np.where(np.diff(voxel))[0]\n",
    "        bounds = np.array([x+1 for x in bounds_perm])\n",
    "        voxel_conv = regressor_to_TR(voxel.reshape(-1,1),tr_duration)\n",
    "        voxel_conv = np.squeeze(voxel_conv)\n",
    "        voxel_conv = np.stack([voxel_conv for _ in range(nSubj)], axis=0).T\n",
    "        \n",
    "        #print(p,feature.shape,feature_conv.shape,len(bounds))\n",
    "\n",
    "    #after perms, get pvalue\n",
    "    pval_corr = brainiak.utils.utils.p_from_null(perm_corrs[0], perm_corrs[1:nPerm+1],side='right',exact=False, axis=None) #\n",
    "    pval_score = brainiak.utils.utils.p_from_null(perm_scores[0], perm_scores[1:nPerm+1],side='right',exact=False, axis=None) \n",
    "    return perm_corrs[0],pval_corr,perm_scores[0],pval_score\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
